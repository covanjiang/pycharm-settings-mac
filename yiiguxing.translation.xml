<application>
  <component name="AppStorage">
    <option name="newTranslationDialogX" value="786" />
    <option name="newTranslationDialogY" value="484" />
    <histories>
      <item value="Content" />
      <item value="supplementary" />
      <item value="compensation" />
      <item value="discrepancies" />
      <item value="There are discrepancies in media consumption, please check your email!" />
      <item value="Media consumption indistinguishable data!" />
      <item value="preamble" />
      <item value="transpose" />
      <item value="embedding" />
      <item value="When the timeout argument is not present or None, the operation will block until the thread terminates. A thread can be join()ed many times. join() raises a RuntimeError if an attempt is made to join the current thread as that would cause a deadlock. It is also an error to join() a thread before it has been started and attempts to do so raises the same exception." />
      <item value="hen the timeout argument is present and not None, it should be a floating point number specifying a timeout for the operation in seconds (or fractions thereof). As join() always returns None, you must call isAlive() after join() to decide whether a timeout happened -- if the thread is still alive, the join() call timed out." />
      <item value="This blocks the calling thread until the thread whose join() method is called terminates -- either normally or through an unhandled exception or until the optional timeout occurs." />
      <item value="statements" />
      <item value="Normally, THttpServer always sends a 200 response. If a handler wants to override this behavior (e.g., to simulate a misconfigured or overloaded web server during testing), it can raise a ResponseException. The function passed to the constructor will be called with the RequestHandler as its only argument. Note that this is irrelevant for ONEWAY requests, as the HTTP response must be sent before the RPC is processed." />
      <item value="propagate" />
      <item value="income" />
      <item value="Domestic" />
      <item value="stats latency" />
      <item value="latency" />
      <item value="accuracy" />
      <item value="A toolset for quality control, evlation and processing of GRID-seq libarary." />
      <item value="known case of datetime.datetime.utcnow &quot;&quot;&quot; Return a new datetime representing UTC day and time." />
      <item value="Unexpected argument&#10;" />
      <item value="Unexpected argument" />
      <item value="The delimiter according which to split the string. None (the default value) means split according to any whitespace, and discard empty strings from the result. maxsplit&#10;" />
      <item value="The delimiter according which to split the string. None (the default value) means split according to any whitespace, and discard empty strings from the result. maxsplit" />
      <item value="Returns the approximate memory footprint an object and all of its contents. Automatically finds the contents of the following builtin containers and their subclasses: tuple, list, deque, dict, set and frozenset. To search other containers, add handlers to iterate over their contents: handlers = {SomeContainerClass: iter, OtherContainerClass: OtherContainerClass.get_elements}" />
      <item value="verbose" />
      <item value="so slow" />
      <item value="Get polyA peaks for given set of depths" />
      <item value="fusion reads" />
      <item value="sorted ids of intervals in cluster(tuples)" />
      <item value="Cluster reads into positive strand and negative strand" />
      <item value="minimal reads to support APAspeaks" />
      <item value="minimal distance between two adjacent APAspeaks" />
      <item value="Shadows name 'split_unit' from outer scope" />
    </histories>
    <option name="languageScores">
      <map>
        <entry key="CHINESE" value="32" />
        <entry key="ENGLISH" value="33" />
      </map>
    </option>
  </component>
  <component name="Cache">
    <option name="lastTrimTime" value="1657510488418" />
  </component>
  <component name="Translation.Cache">
    <option name="lastTrimTime" value="1706514705141" />
  </component>
  <component name="Translation.Settings">
    <option name="keepFormat" value="true" />
    <option name="primaryLanguage" value="CHINESE" />
    <option name="translator" value="GOOGLE" />
  </component>
  <component name="Translation.States">
    <option name="newTranslationDialogX" value="912" />
    <option name="newTranslationDialogY" value="452" />
    <option name="pinTranslationDialog" value="true" />
    <histories>
      <item value="A list of functions can be defined, each containing the name of the function, an optional description, and the parameters the function accepts (described as a JSON schema).&#10;" />
      <item value=" Functions&#10;&#10;With setup and authentication complete, you can now use functions with the Azure OpenAI service. This will be split into a few steps:&#10;&#10;1. Define the function(s)&#10;2. Pass function definition(s) into chat completions API&#10;3. Call function with arguments from the response&#10;4. Feed function response back into chat completions API" />
      <item value="A token is valid for a period of time, after which it will expire. To ensure a valid token is sent with every request, you can refresh an expiring token by hooking into requests.auth:&#10;" />
      <item value=" In this example, we configured the library to use the Azure API by setting the variables in code. For development, consider setting the environment variables instead:" />
      <item value="To set up the OpenAI SDK to use an Azure API Key, we need to set up the `api_type` to `azure` and set `api_key` to a key associated with your endpoint (you can find this key in &quot;Keys and Endpoints&quot; under &quot;Resource Management&quot; in the [Azure Portal](https:portal.azure.com))&#10;" />
      <item value="The Azure OpenAI service supports multiple authentication mechanisms that include API keys and Azure credentials.&#10;" />
      <item value="If you want to use Microsoft Active Directory" />
      <item value="Once the resource is created, the first thing we need to use is its endpoint. You can get the endpoint by looking at the &quot;Keys and Endpoints&quot; section under the &quot;Resource Management&quot; section. Having this, we will set up the SDK using this information:" />
      <item value="Additionally, to properly access the Azure OpenAI Service, we need to create the proper resources at the [Azure Portal](https:portal.azure.com) (you can check a detailed guide on how to do this in the [Microsoft Docs]" />
      <item value="Functions allow a caller of chat completions to define capabilities that the model can use to extend its&#10;functionality into external tools and data sources." />
      <item value="caller" />
      <item value="Please perform the calculation 270737397 then reply with just the integer answer with no commas or anything, nothing else" />
      <item value="Hi, can you test out a bunch of markdown features? Try writing a fenced code block, a table, headers, everything. DO NOT write the markdown inside a markdown code block, just write it raw." />
      <item value="Can you write a single block of code and run_code it that prints something, then delays 1 second, then prints something else? No talk just code. Thanks!" />
      <item value="Can you write a nested for loop in python and shell and run them? Also put 1-3 newlines between each line in the code. Thanks!" />
      <item value="Deduct the system message tokens from the max_tokens if system message exists" />
      <item value="This doesn't go to the LLM. We fill this up w the LLM's response" />
      <item value="Add a new message from the assistant to interpreter's &quot;messages&quot; attribute" />
      <item value="To make a simple app, use HTMLBulma CSSJS. First, plan. Think deeply about the functionality, what the JS will need to do, and how it will need to work with the HTML. Then, all in one `html` code block (DO NOT `run_code` more than once, and NEVER use placeholders like &quot; Javascript code here&quot; -- you're going to write the HTMLJS in one `run_code` function call): Put Bulma CSS and anything else you need in &lt;head&gt;, write the &lt;body&gt; of the app (add lots of padding on the body with Bulma), write the JS into the &lt;script&gt; tag. You probably want to center the app in a box with a border and make sure the body fills up the whole height of the page! Write LOTS of &lt;!--comments--&gt; throughout the HTML and Javascript to the user knows what's going on, and use whitespaceindentation properly. This will automatically open the HTML file simple app on the user's machine." />
      <item value="Open Procedures is an open-source database of tiny, up-to-date coding tutorials." />
      <item value="We can query it semantically and append relevant tutorialsprocedures to our system message" />
      <item value="Yields tokens, but also adds them to interpreter.messages. TBH probably would be good to seperate those two responsibilities someday soon Responds until it decides not to run any more code or say anything else." />
      <item value="In the event we get code -&gt; output -&gt; code again" />
      <item value="We'll use this to determine if we should render a new code block," />
      <item value="Track if we've ran a code block." />
      <item value="Quite different from the plain generator stuff. So redirect to that" />
      <item value="wraps the vanilla .chat(display=False) generator in a display." />
      <item value="Sometimes a little more code -&gt; a much better experience! Display mode actually runs interpreter.chat(display=False, stream=True) from within the terminal_interface. wraps the vanilla .chat(display=False) generator in a display. Quite different from the plain generator stuff. So redirect to that" />
      <item value="Display markdown message. Works with multiline strings with lots of indentation. Will automatically make single line &gt; tags beautiful." />
      <item value="Interactivley prompt the user for required LLM settings" />
      <item value="Idempotency" />
      <item value="this field is required and must take it's default value" />
      <item value="equivalent" />
      <item value="``blocking`` indicates whether calling ``acquire`` should block until the lock has been acquired or to fail immediately, causing ``acquire`` to return False and the lock not being acquired. Defaults to True. Note this value can be overridden by passing a ``blocking`` argument to ``acquire``." />
      <item value="chance" />
      <item value="request" />
      <item value="acquire" />
      <item value="This can be used by a caller to determine whether passing in a list of documents would exceed a certain prompt length. This useful when trying to ensure that the size of a prompt remains below a certain context limit." />
      <item value="Return the prompt length given the documents passed in. This can be used by a caller to determine whether passing in a list of documents would exceed a certain prompt length. This useful when trying to ensure that the size of a prompt remains below a certain context limit." />
      <item value="If the chain expects multiple inputs, they can be passed in directly as keyword arguments." />
      <item value="If the chain expects a single input, it can be passed in as the sole positional argument." />
      <item value="Return docs most similar to query." />
      <item value="Timeout for requests to OpenAI completion API. Default is 600 seconds." />
      <item value="Chain that combines documents by stuffing into context. This chain takes a list of documents and first combines them into a single string. It does this by formatting each document into a string with the `document_prompt` and then joining them together with `document_separator`. It then adds that new string to the inputs with the variable name set by `document_variable_name`. Those inputs are then passed to the `llm_chain`." />
      <item value="Chain to use to combine the documents." />
      <item value="Optional metadata associated with the chain. Defaults to None This metadata will be associated with each call to this chain, and passed as arguments to the handlers defined in `callbacks`. You can use these to eg identify a specific instance of a chain with its use case." />
      <item value="Optional list of tags associated with the chain. Defaults to None These tags will be associated with each call to this chain, and passed as arguments to the handlers defined in `callbacks`. You can use these to eg identify a specific instance of a chain with its use case." />
      <item value="Whether or not run in verbose mode. In verbose mode, some intermediate logs will be printed to the console. Defaults to `langchain.verbose` value." />
      <item value="Optional list of callback handlers (or callback manager). Defaults to None. Callback handlers are called throughout the lifecycle of a call to a chain, starting with on_chain_start, ending with on_chain_end or on_chain_error. Each custom chain can optionally call additional callback methods, see Callback docs for full details." />
      <item value="Optional memory object. Defaults to None. Memory is a class that gets called at the start and at the end of every chain. At the start, memory loads variables and passes them along in the chain. At the end, it saves any returned variables. There are many different types of memory - please see memory docs for the full catalog." />
    </histories>
    <option name="languageScores">
      <map>
        <entry key="CHINESE" value="339" />
        <entry key="ENGLISH" value="342" />
      </map>
    </option>
  </component>
</application>