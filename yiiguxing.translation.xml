<application>
  <component name="AppStorage">
    <option name="newTranslationDialogX" value="786" />
    <option name="newTranslationDialogY" value="484" />
    <histories>
      <item value="Content" />
      <item value="supplementary" />
      <item value="compensation" />
      <item value="discrepancies" />
      <item value="There are discrepancies in media consumption, please check your email!" />
      <item value="Media consumption indistinguishable data!" />
      <item value="preamble" />
      <item value="transpose" />
      <item value="embedding" />
      <item value="When the timeout argument is not present or None, the operation will block until the thread terminates. A thread can be join()ed many times. join() raises a RuntimeError if an attempt is made to join the current thread as that would cause a deadlock. It is also an error to join() a thread before it has been started and attempts to do so raises the same exception." />
      <item value="hen the timeout argument is present and not None, it should be a floating point number specifying a timeout for the operation in seconds (or fractions thereof). As join() always returns None, you must call isAlive() after join() to decide whether a timeout happened -- if the thread is still alive, the join() call timed out." />
      <item value="This blocks the calling thread until the thread whose join() method is called terminates -- either normally or through an unhandled exception or until the optional timeout occurs." />
      <item value="statements" />
      <item value="Normally, THttpServer always sends a 200 response. If a handler wants to override this behavior (e.g., to simulate a misconfigured or overloaded web server during testing), it can raise a ResponseException. The function passed to the constructor will be called with the RequestHandler as its only argument. Note that this is irrelevant for ONEWAY requests, as the HTTP response must be sent before the RPC is processed." />
      <item value="propagate" />
      <item value="income" />
      <item value="Domestic" />
      <item value="stats latency" />
      <item value="latency" />
      <item value="accuracy" />
      <item value="A toolset for quality control, evlation and processing of GRID-seq libarary." />
      <item value="known case of datetime.datetime.utcnow &quot;&quot;&quot; Return a new datetime representing UTC day and time." />
      <item value="Unexpected argument&#10;" />
      <item value="Unexpected argument" />
      <item value="The delimiter according which to split the string. None (the default value) means split according to any whitespace, and discard empty strings from the result. maxsplit&#10;" />
      <item value="The delimiter according which to split the string. None (the default value) means split according to any whitespace, and discard empty strings from the result. maxsplit" />
      <item value="Returns the approximate memory footprint an object and all of its contents. Automatically finds the contents of the following builtin containers and their subclasses: tuple, list, deque, dict, set and frozenset. To search other containers, add handlers to iterate over their contents: handlers = {SomeContainerClass: iter, OtherContainerClass: OtherContainerClass.get_elements}" />
      <item value="verbose" />
      <item value="so slow" />
      <item value="Get polyA peaks for given set of depths" />
      <item value="fusion reads" />
      <item value="sorted ids of intervals in cluster(tuples)" />
      <item value="Cluster reads into positive strand and negative strand" />
      <item value="minimal reads to support APAspeaks" />
      <item value="minimal distance between two adjacent APAspeaks" />
      <item value="Shadows name 'split_unit' from outer scope" />
    </histories>
    <option name="languageScores">
      <map>
        <entry key="CHINESE" value="32" />
        <entry key="ENGLISH" value="33" />
      </map>
    </option>
  </component>
  <component name="Cache">
    <option name="lastTrimTime" value="1657510488418" />
  </component>
  <component name="Translation.Cache">
    <option name="lastTrimTime" value="1706514705141" />
  </component>
  <component name="Translation.Settings">
    <option name="keepFormat" value="true" />
    <option name="primaryLanguage" value="CHINESE" />
    <option name="translator" value="GOOGLE" />
  </component>
  <component name="Translation.States">
    <option name="newTranslationDialogX" value="912" />
    <option name="newTranslationDialogY" value="452" />
    <option name="pinTranslationDialog" value="true" />
    <histories>
      <item value="If you want to use Microsoft Active Directory" />
      <item value="Once the resource is created, the first thing we need to use is its endpoint. You can get the endpoint by looking at the &quot;Keys and Endpoints&quot; section under the &quot;Resource Management&quot; section. Having this, we will set up the SDK using this information:" />
      <item value="Additionally, to properly access the Azure OpenAI Service, we need to create the proper resources at the [Azure Portal](https:portal.azure.com) (you can check a detailed guide on how to do this in the [Microsoft Docs]" />
      <item value="Functions allow a caller of chat completions to define capabilities that the model can use to extend its&#10;functionality into external tools and data sources." />
      <item value="caller" />
      <item value="Please perform the calculation 270737397 then reply with just the integer answer with no commas or anything, nothing else" />
      <item value="Hi, can you test out a bunch of markdown features? Try writing a fenced code block, a table, headers, everything. DO NOT write the markdown inside a markdown code block, just write it raw." />
      <item value="Can you write a single block of code and run_code it that prints something, then delays 1 second, then prints something else? No talk just code. Thanks!" />
      <item value="Can you write a nested for loop in python and shell and run them? Also put 1-3 newlines between each line in the code. Thanks!" />
      <item value="Deduct the system message tokens from the max_tokens if system message exists" />
      <item value="This doesn't go to the LLM. We fill this up w the LLM's response" />
      <item value="Add a new message from the assistant to interpreter's &quot;messages&quot; attribute" />
      <item value="To make a simple app, use HTMLBulma CSSJS. First, plan. Think deeply about the functionality, what the JS will need to do, and how it will need to work with the HTML. Then, all in one `html` code block (DO NOT `run_code` more than once, and NEVER use placeholders like &quot; Javascript code here&quot; -- you're going to write the HTMLJS in one `run_code` function call): Put Bulma CSS and anything else you need in &lt;head&gt;, write the &lt;body&gt; of the app (add lots of padding on the body with Bulma), write the JS into the &lt;script&gt; tag. You probably want to center the app in a box with a border and make sure the body fills up the whole height of the page! Write LOTS of &lt;!--comments--&gt; throughout the HTML and Javascript to the user knows what's going on, and use whitespaceindentation properly. This will automatically open the HTML file simple app on the user's machine." />
      <item value="Open Procedures is an open-source database of tiny, up-to-date coding tutorials." />
      <item value="We can query it semantically and append relevant tutorialsprocedures to our system message" />
      <item value="Yields tokens, but also adds them to interpreter.messages. TBH probably would be good to seperate those two responsibilities someday soon Responds until it decides not to run any more code or say anything else." />
      <item value="In the event we get code -&gt; output -&gt; code again" />
      <item value="We'll use this to determine if we should render a new code block," />
      <item value="Track if we've ran a code block." />
      <item value="Quite different from the plain generator stuff. So redirect to that" />
      <item value="wraps the vanilla .chat(display=False) generator in a display." />
      <item value="Sometimes a little more code -&gt; a much better experience! Display mode actually runs interpreter.chat(display=False, stream=True) from within the terminal_interface. wraps the vanilla .chat(display=False) generator in a display. Quite different from the plain generator stuff. So redirect to that" />
      <item value="Display markdown message. Works with multiline strings with lots of indentation. Will automatically make single line &gt; tags beautiful." />
      <item value="Interactivley prompt the user for required LLM settings" />
      <item value="Idempotency" />
      <item value="this field is required and must take it's default value" />
      <item value="equivalent" />
      <item value="``blocking`` indicates whether calling ``acquire`` should block until the lock has been acquired or to fail immediately, causing ``acquire`` to return False and the lock not being acquired. Defaults to True. Note this value can be overridden by passing a ``blocking`` argument to ``acquire``." />
      <item value="chance" />
      <item value="request" />
      <item value="acquire" />
      <item value="This can be used by a caller to determine whether passing in a list of documents would exceed a certain prompt length. This useful when trying to ensure that the size of a prompt remains below a certain context limit." />
      <item value="Return the prompt length given the documents passed in. This can be used by a caller to determine whether passing in a list of documents would exceed a certain prompt length. This useful when trying to ensure that the size of a prompt remains below a certain context limit." />
      <item value="If the chain expects multiple inputs, they can be passed in directly as keyword arguments." />
      <item value="If the chain expects a single input, it can be passed in as the sole positional argument." />
      <item value="Return docs most similar to query." />
      <item value="Timeout for requests to OpenAI completion API. Default is 600 seconds." />
      <item value="Chain that combines documents by stuffing into context. This chain takes a list of documents and first combines them into a single string. It does this by formatting each document into a string with the `document_prompt` and then joining them together with `document_separator`. It then adds that new string to the inputs with the variable name set by `document_variable_name`. Those inputs are then passed to the `llm_chain`." />
      <item value="Chain to use to combine the documents." />
      <item value="Optional metadata associated with the chain. Defaults to None This metadata will be associated with each call to this chain, and passed as arguments to the handlers defined in `callbacks`. You can use these to eg identify a specific instance of a chain with its use case." />
      <item value="Optional list of tags associated with the chain. Defaults to None These tags will be associated with each call to this chain, and passed as arguments to the handlers defined in `callbacks`. You can use these to eg identify a specific instance of a chain with its use case." />
      <item value="Whether or not run in verbose mode. In verbose mode, some intermediate logs will be printed to the console. Defaults to `langchain.verbose` value." />
      <item value="Optional list of callback handlers (or callback manager). Defaults to None. Callback handlers are called throughout the lifecycle of a call to a chain, starting with on_chain_start, ending with on_chain_end or on_chain_error. Each custom chain can optionally call additional callback methods, see Callback docs for full details." />
      <item value="Optional memory object. Defaults to None. Memory is a class that gets called at the start and at the end of every chain. At the start, memory loads variables and passes them along in the chain. At the end, it saves any returned variables. There are many different types of memory - please see memory docs for the full catalog." />
      <item value="Abstract base class for creating structured sequences of calls to components. Chains should be used to encode a sequence of calls to components like models, document retrievers, other chains, etc., and provide a simple interface to this sequence. The Chain interface makes it easy to create apps that are: - Stateful: add Memory to any Chain to give it state, - Observable: pass Callbacks to a Chain to execute additional functionality, like logging, outside the main sequence of component calls, - Composable: the Chain API is flexible enough that it is easy to combine Chains with other components, including other Chains. The main methods exposed by chains are: - `__call__`: Chains are callable. The `__call__` method is the primary way to execute a Chain. This takes inputs as a dictionary and returns a dictionary output. - `run`: A convenience method that takes inputs as argskwargs and returns the output as a string. This method can only be used for a subset of chains and cannot return as rich of an output as `__call__`." />
      <item value="Calculate num tokens for gpt-3.5-turbo and gpt-4 with tiktoken package. Official documentation: https:github.comopenaiopenai-cookbookblob mainexamplesHow_to_format_inputs_to_ChatGPT_models.ipynb" />
      <item value="Number of chat completions to generate for each prompt.&quot;" />
      <item value="Pass a sequence of prompts to the model and return model generations. This method should make use of batched calls for models that expose a batched API. Use this method when you want to: 1. take advantage of batched calls, 2. need more output from the model than just the top generated value, 3. are building chains that are agnostic to the underlying language model type (e.g., pure text completion models vs chat models). Args: prompts: List of PromptValues. A PromptValue is an object that can be converted to match the format of any language model (string for pure text generation models and BaseMessages for chat models). stop: Stop words to use when generating. Model output is cut off at the first occurrence of any of these substrings. callbacks: Callbacks to pass through. Used for executing additional functionality, such as logging or streaming, throughout generation. kwargs: Arbitrary additional keyword arguments. These are usually passed to the model provider API call. Returns: An LLMResult, which contains a list of candidate Generations for each input prompt and additional model provider-specific output." />
      <item value="We hope to increase the number of inputs per request soon" />
      <item value="It is possible to do more validation of the headers, but a request could always be made to the API manually with invalid headers, so we need to handle them server side." />
    </histories>
    <option name="languageScores">
      <map>
        <entry key="CHINESE" value="333" />
        <entry key="ENGLISH" value="336" />
      </map>
    </option>
  </component>
</application>