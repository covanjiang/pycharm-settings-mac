<application>
  <component name="AppStorage">
    <option name="newTranslationDialogX" value="786" />
    <option name="newTranslationDialogY" value="484" />
    <histories>
      <item value="Content" />
      <item value="supplementary" />
      <item value="compensation" />
      <item value="discrepancies" />
      <item value="There are discrepancies in media consumption, please check your email!" />
      <item value="Media consumption indistinguishable data!" />
      <item value="preamble" />
      <item value="transpose" />
      <item value="embedding" />
      <item value="When the timeout argument is not present or None, the operation will block until the thread terminates. A thread can be join()ed many times. join() raises a RuntimeError if an attempt is made to join the current thread as that would cause a deadlock. It is also an error to join() a thread before it has been started and attempts to do so raises the same exception." />
      <item value="hen the timeout argument is present and not None, it should be a floating point number specifying a timeout for the operation in seconds (or fractions thereof). As join() always returns None, you must call isAlive() after join() to decide whether a timeout happened -- if the thread is still alive, the join() call timed out." />
      <item value="This blocks the calling thread until the thread whose join() method is called terminates -- either normally or through an unhandled exception or until the optional timeout occurs." />
      <item value="statements" />
      <item value="Normally, THttpServer always sends a 200 response. If a handler wants to override this behavior (e.g., to simulate a misconfigured or overloaded web server during testing), it can raise a ResponseException. The function passed to the constructor will be called with the RequestHandler as its only argument. Note that this is irrelevant for ONEWAY requests, as the HTTP response must be sent before the RPC is processed." />
      <item value="propagate" />
      <item value="income" />
      <item value="Domestic" />
      <item value="stats latency" />
      <item value="latency" />
      <item value="accuracy" />
      <item value="A toolset for quality control, evlation and processing of GRID-seq libarary." />
      <item value="known case of datetime.datetime.utcnow &quot;&quot;&quot; Return a new datetime representing UTC day and time." />
      <item value="Unexpected argument&#10;" />
      <item value="Unexpected argument" />
      <item value="The delimiter according which to split the string. None (the default value) means split according to any whitespace, and discard empty strings from the result. maxsplit&#10;" />
      <item value="The delimiter according which to split the string. None (the default value) means split according to any whitespace, and discard empty strings from the result. maxsplit" />
      <item value="Returns the approximate memory footprint an object and all of its contents. Automatically finds the contents of the following builtin containers and their subclasses: tuple, list, deque, dict, set and frozenset. To search other containers, add handlers to iterate over their contents: handlers = {SomeContainerClass: iter, OtherContainerClass: OtherContainerClass.get_elements}" />
      <item value="verbose" />
      <item value="so slow" />
      <item value="Get polyA peaks for given set of depths" />
      <item value="fusion reads" />
      <item value="sorted ids of intervals in cluster(tuples)" />
      <item value="Cluster reads into positive strand and negative strand" />
      <item value="minimal reads to support APAspeaks" />
      <item value="minimal distance between two adjacent APAspeaks" />
      <item value="Shadows name 'split_unit' from outer scope" />
    </histories>
    <option name="languageScores">
      <map>
        <entry key="CHINESE" value="32" />
        <entry key="ENGLISH" value="33" />
      </map>
    </option>
  </component>
  <component name="Cache">
    <option name="lastTrimTime" value="1657510488418" />
  </component>
  <component name="Translation.Cache">
    <option name="lastTrimTime" value="1688461856310" />
  </component>
  <component name="Translation.States">
    <option name="newTranslationDialogX" value="912" />
    <option name="newTranslationDialogY" value="452" />
    <option name="pinTranslationDialog" value="true" />
    <histories>
      <item value="Run the logic of this chain and add to output if desired. Args: inputs: Dictionary of inputs, or single input if chain expects only one param. return_only_outputs: boolean for whether to return only outputs in the response. If True, only new keys generated by this chain will be returned. If False, both input keys and new keys generated by this chain will be returned. Defaults to False. callbacks: Callbacks to use for this chain run. If not provided, will use the callbacks provided to the chain. tags: Optional list of tags associated with the chain. Defaults to None metadata: Optional metadata associated with the chain. Defaults to None include_run_info: Whether to include run info in the response. Defaults to False." />
      <item value="Wrapper around Azure OpenAI Chat Completion API. To use this class you must have a deployed model on Azure OpenAI. Use `deployment_name` in the constructor to refer to the &quot;Model deployment name&quot; in the Azure portal." />
      <item value="The model name to pass to tiktoken when using this class. Tiktoken is used to count the number of tokens in documents to constrain them to be under a certain limit. By default, when set to None, this will be the same as the embedding model name. However, there are some cases where you may want to use this Embedding class with a model name not supported by tiktoken. This can include when using Azure embeddings or when using one of the many model providers that expose an OpenAI-like API but with different models. In those cases, in order to avoid erroring when tiktoken is called, you can specify a model name to use here." />
      <item value="Penalizes repeated tokens according to frequency." />
      <item value="Total probability mass of tokens to consider at each step." />
      <item value="The maximum number of tokens to generate in the completion. -1 returns as many tokens as possible given the prompt and the models maximal context size." />
      <item value="Pears are either red or orange" />
      <item value="it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv" />
      <item value="How to determine the score: - Higher is a better answer - Better responds fully to the asked question, with sufficient level of detail - If you do not know the answer based on the context, that should be a score of 0 - Don't be overconfident!" />
      <item value="In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:" />
      <item value="If ``ensure_ascii`` is false, then the return value can contain non-ASCII characters if they appear in strings contained in ``obj``. Otherwise, all such characters are escaped in JSON strings." />
      <item value="This is a temporary workaround to make the similarity search asynchronous. The proper solution is to make the similarity search asynchronous in the vector store implementations." />
      <item value="&quot;&quot;&quot;Run the logic of this chain and add to output if desired. Args: inputs: Dictionary of inputs, or single input if chain expects only one param. return_only_outputs: boolean for whether to return only outputs in the response. If True, only new keys generated by this chain will be returned. If False, both input keys and new keys generated by this chain will be returned. Defaults to False. callbacks: Callbacks to use for this chain run. If not provided, will use the callbacks provided to the chain. include_run_info: Whether to include run info in the response. Defaults to False. &quot;&quot;&quot;" />
      <item value="close: if left at its default of ``True``, has the effect of fully closing all currently checked in database connections. Connections that are still checked out will not be closed, however they will no longer be associated with this :class:`_engine.Engine`, so when they are closed individually, eventually the :class:`_pool.Pool` which they are associated with will be garbage collected and they will be closed out fully, if not already closed on checkin." />
      <item value="Split each pair by one or more newline characters possibly surrounded by whitespaces" />
      <item value="Split the content by two or more newline characters possibly surrounded by whitespaces" />
      <item value="Returns an active Redis client generated from the given database URL. Will attempt to extract the database id from the path url fragment, if none is provided." />
      <item value="You might need to adjust how you initialize your database, feishu client, and vector database, based on how FastAPI handles asynchronous operations." />
      <item value="PEP 8: E741 ambiguous variable name 'l'" />
      <item value="Batch size to use when passing multiple documents to generate." />
      <item value="Holds any model parameters valid for `create` call not explicitly specified." />
      <item value="Generates best_of completions server-side and returns the &quot;best&quot;." />
      <item value="How many completions to generate for each prompt." />
      <item value="Penalizes repeated tokens." />
      <item value="Given the context information and not prior knowledge, answer the question" />
      <item value="&quot;&quot;&quot;Wrapper around Azure-specific OpenAI large language models. To use, you should have the ``openai`` python package installed, and the environment variable ``OPENAI_API_KEY`` set with your API key. Any parameters that are valid to be passed to the openai.create call can be passed in, even if not explicitly saved on this class. Example: .. code-block:: python from langchain.llms import AzureOpenAI openai = AzureOpenAI(model_name=&quot;text-davinci-003&quot;) &quot;&quot;&quot;" />
      <item value="Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer." />
      <item value="Call predict on the LLM." />
      <item value="A rate limit error will occur when API requests are sent too quickly. If using the OpenAI Python library, they will look something like:" />
      <item value="Rate limits are a common practice for APIs, and they're put in place for a few different reasons. - First, they help protect against abuse or misuse of the API. For example, a malicious actor could flood the API with requests in an attempt to overload it or cause disruptions in service. By setting rate limits, OpenAI can prevent this kind of activity. - Second, rate limits help ensure that everyone has fair access to the API. If one person or organization makes an excessive number of requests, it could bog down the API for everyone else. By throttling the number of requests that a single user can make, OpenAI ensures that everyone has an opportunity to use the API without experiencing slowdowns. - Lastly, rate limits can help OpenAI manage the aggregate load on its infrastructure. If requests to the API increase dramatically, it could tax the servers and cause performance issues. By setting rate limits, OpenAI can help maintain a smooth and consistent experience for all users. Although hitting rate limits can be frustrating, rate limits exist to protect the reliable operation of the API for its users." />
      <item value="To see an example script for throttling parallel requests to avoid rate limit errors, see [api_request_parallel_processor.py](api_request_parallel_processor.py)." />
      <item value="This guide shares tips for avoiding and handling rate limit errors." />
      <item value="When you call the OpenAI API repeatedly, you may encounter error messages that say `429: 'Too Many Requests'` or `RateLimitError`. These error messages come from exceeding the API's rate limits." />
      <item value="How to handle rate limits" />
      <item value="Techniques to improve reliability" />
      <item value="VectorStore implementation using Postgres and pgvector. - `connection_string` is a postgres connection string. - `embedding_function` any embedding function implementing `langchain.embeddings.base.Embeddings` interface. - `collection_name` is the name of the collection to use. (default: langchain) - NOTE: This is not the name of the table, but the name of the collection. The tables will be created when initializing the store (if not exists) So, make sure the user has the right permissions to create tables. - `distance_strategy` is the distance strategy to use. (default: EUCLIDEAN) - `EUCLIDEAN` is the euclidean distance. - `COSINE` is the cosine distance. - `pre_delete_collection` if True, will delete the collection if it exists. (default: False) - Useful for testing." />
      <item value="Deprecation of Answers, Classification, and Search" />
      <item value="&quot;&quot;&quot; The preferred method to get a ClickHouse Connect Client instance :param host: The hostname or IP address of the ClickHouse server. If not set, localhost will be used. :param username: The ClickHouse username. If not set, the default ClickHouse user will be used. :param password: The password for username. :param database: The default database for the connection. If not set, ClickHouse Connect will use the default database for username. :param interface: Must be http or https. Defaults to http, or to https if port is set to 8443 or 443 :param port: The ClickHouse HTTP or HTTPS port. If not set will default to 8123, or to 8443 if secure=True or interface=https. :param secure: Use httpsTLS. This overrides inferred values from the interface or port arguments. :param dsn: A string in standard DSN (Data Source Name) format. Other connection values (such as host or user) will be extracted from this string if not set otherwise. :param settings: ClickHouse server settings to be used with the sessionevery request :param generic_args: Used internally to parse DBAPI connection strings into keyword arguments and ClickHouse settings. It is not recommended to use this parameter externally. :param kwargs -- Recognized keyword arguments (used by the HTTP client), see below :param compress: Enable compression for ClickHouse HTTP inserts and query results. True will select the preferred compression method (lz4). A str of 'lz4', 'zstd', 'brotli', or 'gzip' can be used to use a specific compression type :param query_limit: Default LIMIT on returned rows. 0 means no limit :param connect_timeout: Timeout in seconds for the http connection :param send_receive_timeout: Read timeout in seconds for http connection :param client_name: client_name prepended to the HTTP User Agent header. Set this to track client queries in the ClickHouse system.query_log. :param send_progress: Deprecated, has no effect. Previous functionality is now automatically determined :param verify: Verify the server certificate in securehttps mode :param ca_cert: If verify is True, the file path to Certificate Authority root to validate ClickHouse server certificate, in .pem format. Ignored if verify is False. This is not necessary if the ClickHouse server certificate is trusted by the operating system. To trust the maintained list of &quot;global&quot; public root certificates maintained by the Python 'certifi' package, set ca_cert to 'certifi' :param client_cert: File path to a TLS Client certificate in .pem format. This file should contain any applicable intermediate certificates :param client_cert_key: File path to the private key for the Client Certificate. Required if the private key is not included the Client Certificate key file :param session_id ClickHouse session id. If not specified and the common setting 'autogenerate_session_id' is True, the client will generate a UUID1 session id :param pool_mgr Optional urllib3 PoolManager for this client. Useful for creating separate connection pools for multiple client endpoints for applications with many clients :param http_proxy http proxy address. Equivalent to setting the HTTP_PROXY environment variable :param https_proxy https proxy address. Equivalent to setting the HTTPS_PROXY environment variable :param server_host_name This is the server host name that will be checked against a TLS certificate for validity. This option can be used if using an ssh_tunnel or other indirect means to an ClickHouse server where the `host` argument refers to the tunnel or proxy and not the actual ClickHouse server :return: ClickHouse Connect Client instance" />
      <item value="&quot;&quot;&quot;Call out to OpenAI's endpoint with k unique prompts. Args: prompts: The prompts to pass into the model. stop: Optional list of stop words to use when generating. Returns: The full LLM output. Example: .. code-block:: python response = openai.generate([&quot;Tell me a joke.&quot;]) &quot;&quot;&quot; TODO: write a unit test for this" />
      <item value="Remove plural from include since db columns are singular" />
      <item value="Either the collection name or the collection uuid must be provided" />
      <item value="&quot;&quot;&quot;Gets the nearest neighbors of a single embedding ⚠️ This method should not be used directly. Args: embedding (Sequence[float]): The embedding to find the nearest neighbors of n_results (int, optional): The number of nearest neighbors to return. Defaults to 10. where (Dict[str, str], optional): A dictionary of key-value pairs to filter the embeddings by. Defaults to {}. &quot;&quot;&quot;" />
      <item value="Infers if target is Embedding, Metadata, or Document and casts it to a many object if its one" />
      <item value="&quot;&quot;&quot;Get the n_results nearest neighbor embeddings for provided query_embeddings or query_texts. Args: query_embeddings: The embeddings to get the closes neighbors of. Optional. query_texts: The document texts to get the closes neighbors of. Optional. n_results: The number of neighbors to return for each query_embedding or query_text. Optional. where: A Where type dict used to filter results by. E.g. {&quot;color&quot; : &quot;red&quot;, &quot;price&quot;: 4.20}. Optional. where_document: A WhereDocument type dict used to filter by the documents. E.g. {contains: {&quot;text&quot;: &quot;hello&quot;}}. Optional. include: A list of what to include in the results. Can contain &quot;embeddings&quot;, &quot;metadatas&quot;, &quot;documents&quot;, &quot;distances&quot;. Ids are always included. Defaults to [&quot;metadatas&quot;, &quot;documents&quot;, &quot;distances&quot;]. Optional. &quot;&quot;&quot;" />
      <item value="&quot;&quot;&quot;Return docs and relevance scores in the range [0, 1]. 0 is dissimilar, 1 is most similar. Args: query: input text k: Number of Documents to return. Defaults to 4. kwargs: kwargs to be passed to similarity search. Should include: score_threshold: Optional, a floating point value between 0 to 1 to filter the resulting set of retrieved docs Returns: List of Tuples of (doc, similarity_score) &quot;&quot;&quot;" />
      <item value="&quot;&quot;&quot;Run get_relevant_text and llm on input query. If chain has 'return_source_documents' as 'True', returns the retrieved documents as well under the key 'source_documents'. Example: .. code-block:: python res = indexqa({'query': 'This is my query'}) answer, docs = res['result'], res['source_documents'] &quot;&quot;&quot;" />
      <item value="generate" />
      <item value="Use the following pieces of context to answer the users question. If you don't know the answer, just say that you don't know, don't try to make up an answer. ---------------- {context}" />
      <item value="&quot;&quot;&quot;Load question answering chain. Args: llm: Language Model to use in the chain. chain_type: Type of document combining chain to use. Should be one of &quot;stuff&quot;, &quot;map_reduce&quot;, &quot;map_rerank&quot;, and &quot;refine&quot;. verbose: Whether chains should be run in verbose mode or not. Note that this applies to all chains that make up the final chain. callback_manager: Callback manager to use for the chain. Returns: A chain to use for question answering. &quot;&quot;&quot;" />
      <item value="&quot;You are trying to use a chat model. This way of initializing it is &quot; &quot;no longer supported. Instead, please use: &quot; &quot;`from langchain.chat_models import ChatOpenAI`&quot;" />
    </histories>
    <option name="languageScores">
      <map>
        <entry key="CHINESE" value="185" />
        <entry key="ENGLISH" value="187" />
      </map>
    </option>
  </component>
</application>