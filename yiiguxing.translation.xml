<application>
  <component name="AppStorage">
    <option name="newTranslationDialogX" value="786" />
    <option name="newTranslationDialogY" value="484" />
    <histories>
      <item value="Content" />
      <item value="supplementary" />
      <item value="compensation" />
      <item value="discrepancies" />
      <item value="There are discrepancies in media consumption, please check your email!" />
      <item value="Media consumption indistinguishable data!" />
      <item value="preamble" />
      <item value="transpose" />
      <item value="embedding" />
      <item value="When the timeout argument is not present or None, the operation will block until the thread terminates. A thread can be join()ed many times. join() raises a RuntimeError if an attempt is made to join the current thread as that would cause a deadlock. It is also an error to join() a thread before it has been started and attempts to do so raises the same exception." />
      <item value="hen the timeout argument is present and not None, it should be a floating point number specifying a timeout for the operation in seconds (or fractions thereof). As join() always returns None, you must call isAlive() after join() to decide whether a timeout happened -- if the thread is still alive, the join() call timed out." />
      <item value="This blocks the calling thread until the thread whose join() method is called terminates -- either normally or through an unhandled exception or until the optional timeout occurs." />
      <item value="statements" />
      <item value="Normally, THttpServer always sends a 200 response. If a handler wants to override this behavior (e.g., to simulate a misconfigured or overloaded web server during testing), it can raise a ResponseException. The function passed to the constructor will be called with the RequestHandler as its only argument. Note that this is irrelevant for ONEWAY requests, as the HTTP response must be sent before the RPC is processed." />
      <item value="propagate" />
      <item value="income" />
      <item value="Domestic" />
      <item value="stats latency" />
      <item value="latency" />
      <item value="accuracy" />
      <item value="A toolset for quality control, evlation and processing of GRID-seq libarary." />
      <item value="known case of datetime.datetime.utcnow &quot;&quot;&quot; Return a new datetime representing UTC day and time." />
      <item value="Unexpected argument&#10;" />
      <item value="Unexpected argument" />
      <item value="The delimiter according which to split the string. None (the default value) means split according to any whitespace, and discard empty strings from the result. maxsplit&#10;" />
      <item value="The delimiter according which to split the string. None (the default value) means split according to any whitespace, and discard empty strings from the result. maxsplit" />
      <item value="Returns the approximate memory footprint an object and all of its contents. Automatically finds the contents of the following builtin containers and their subclasses: tuple, list, deque, dict, set and frozenset. To search other containers, add handlers to iterate over their contents: handlers = {SomeContainerClass: iter, OtherContainerClass: OtherContainerClass.get_elements}" />
      <item value="verbose" />
      <item value="so slow" />
      <item value="Get polyA peaks for given set of depths" />
      <item value="fusion reads" />
      <item value="sorted ids of intervals in cluster(tuples)" />
      <item value="Cluster reads into positive strand and negative strand" />
      <item value="minimal reads to support APAspeaks" />
      <item value="minimal distance between two adjacent APAspeaks" />
      <item value="Shadows name 'split_unit' from outer scope" />
    </histories>
    <option name="languageScores">
      <map>
        <entry key="CHINESE" value="32" />
        <entry key="ENGLISH" value="33" />
      </map>
    </option>
  </component>
  <component name="Cache">
    <option name="lastTrimTime" value="1657510488418" />
  </component>
  <component name="Translation.Cache">
    <option name="lastTrimTime" value="1686211423219" />
  </component>
  <component name="Translation.States">
    <option name="pinTranslationDialog" value="true" />
    <histories>
      <item value="should this return the ids of the succesfully added items?" />
      <item value="replace newlines, which can negatively affect performance." />
      <item value="Use tenacity to retry the embedding call" />
      <item value="The chunk size of embeddings. If None, will use the chunk size specified by the class." />
      <item value="Returns: List[str]: List of IDs of the added texts." />
      <item value="Args: texts (Iterable[str]): Texts to add to the vectorstore. metadatas (Optional[List[dict]], optional): Optional list of metadatas. ids (Optional[List[str]], optional): Optional list of IDs." />
      <item value="Run more texts through the embeddings and add to the vectorstore." />
      <item value="&quot;&quot;&quot;Run more texts through the embeddings and add to the vectorstore. Args: texts (Iterable[str]): Texts to add to the vectorstore. metadatas (Optional[List[dict]], optional): Optional list of metadatas. ids (Optional[List[str]], optional): Optional list of IDs. Returns: List[str]: List of IDs of the added texts. &quot;&quot;&quot; TODO: Handle the case where the user doesn't provide ids on the Collection" />
      <item value="TODO: inherits ClickHouse for convenience of copying behavior, not because it's logically a subtype. Factoring out the common behavior to a third superclass they both extend would be preferable." />
      <item value="poor man's unique constraint" />
      <item value="Create a Chroma vectorstore from a raw documents. If a persist_directory is specified, the collection will be persisted there. Otherwise, the data will be ephemeral in-memory. Args: texts (List[str]): List of texts to add to the collection. collection_name (str): Name of the collection to create. persist_directory (Optional[str]): Directory to persist the collection. embedding (Optional[Embeddings]): Embedding function. Defaults to None. metadatas (Optional[List[dict]]): List of metadatas. Defaults to None. ids (Optional[List[str]]): List of document IDs. Defaults to None. client_settings (Optional[chromadb.config.Settings]): Chroma client settings Returns: Chroma: Chroma vectorstore." />
      <item value="Maximum number of texts to embed in each batch" />
      <item value="Wrapper around OpenAI embedding models. To use, you should have the ``openai`` python package installed, and the environment variable ``OPENAI_API_KEY`` set with your API key or pass it as a named parameter to the constructor. Example: .. code-block:: python from langchain.embeddings import OpenAIEmbeddings openai = OpenAIEmbeddings(openai_api_key=&quot;my-api-key&quot;) In order to use the library with Microsoft Azure endpoints, you need to set the OPENAI_API_TYPE, OPENAI_API_BASE, OPENAI_API_KEY and optionally and API_VERSION. The OPENAI_API_TYPE must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the model parameter. Example: .. code-block:: python import os os.environ[&quot;OPENAI_API_TYPE&quot;] = &quot;azure&quot; os.environ[&quot;OPENAI_API_BASE&quot;] = &quot;https:&lt;your-endpoint.openai.azure.com&quot; os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;your AzureOpenAI key&quot; from langchain.embeddings.openai import OpenAIEmbeddings embeddings = OpenAIEmbeddings( deployment=&quot;your-embeddings-deployment-name&quot;, model=&quot;your-embeddings-model-name&quot; ) text = &quot;This is a test query.&quot; query_result = embeddings.embed_query(text)" />
      <item value="The Chatbot Kickstarter is a starter repo to get you used to building basic a basic Chatbot using the ChatGPT API and your own knowledge base. The flow you're taken through was originally presented with [these slides](https:drive.google.comfiled1dB-RQhZC_Q1iAsHkNNdkqtxxXqYODFYyview?usp=share_link), which may come in useful to refer to." />
      <item value="The Chatbot Kickstarter is a starter repo to get you used to building basic a basic Chatbot using the ChatGPT API and your own knowledge base. The flow you're taken through was originally presented with [these slides](https:drive.google." />
      <item value="The expected output is the translated chunk of text." />
      <item value="The format of the prompt sent to the model consists of: 1. A high level instruction to translate only the text, but not commands into the desired language 2. A sample untranslated command, where only the content of the chapter name needs to be translated 3. The chunk of text to be translated 4. The translated sample command from 2, which shows the model the beginning of the translation process" />
      <item value="Notice that adding a sample untranslated and translated first command, where only the content of the chapter name needs to be translated, helps to get more consistent results." />
      <item value="if adding this chunk would exceed the max length, finalize the current batch and start a new one" />
      <item value="Group very short chunks, to form approximately a page long chunks." />
      <item value="It turns out that a double newline is a good separator in this case, in order not to break the flow of the text. Also no individual chunk is larger than 1500 tokens. The model we will use is text-davinci-002, which has a limit of 4096 tokens, so we don't need to worry about breaking the chunks down further. We will group the shorter chunks into chunks of around 1000 tokens, to increase the coherence of the text, and decrease the frequency of breaks within the text." />
      <item value="With permission of the author, we will demonstrate how to translate the book [Euclidean Plane Geometry](https:sites.google.comsiteprojektivna), written by Milan Mitrović from Slovenian into English, without modifying any of the LaTeX commands. To achieve this, we will first split the book into chunks, each roughly a page long, then translate each chunk into English, and finally stitch them back together." />
      <item value="Translate a book writen in LaTeX from Slovenian into English" />
      <item value="Note that the exact way that tokens are counted from messages may change from model to model. Consider the counts from the function below an estimate, not a timeless guarantee." />
      <item value="ChatGPT models like `gpt-3.5-turbo` and `gpt-4` use tokens in the same way as older completions models, but because of their message-based formatting, it's more difficult to count how many tokens will be used by a conversation." />
      <item value="Below is an example function for counting tokens for messages passed to `gpt-3.5-turbo-0301` or `gpt-4-0314`." />
      <item value="Different encodings vary in how they split words, group spaces, and handle non-English characters. Using the methods above, we can compare different encodings on a few example strings." />
      <item value="(The `b` in front of the strings indicates that the strings are byte strings.)" />
      <item value="For single tokens, `.decode_single_token_bytes()` safely converts a single integer token to the bytes it represents." />
      <item value="Warning: although `.decode()` can be applied to single tokens, beware that it can be lossy for tokens that aren't on utf-8 boundaries." />
      <item value="Count tokens by counting the length of the list returned by `.encode()`." />
      <item value="to automatically load the correct encoding for a given model name." />
      <item value="The first time this runs, it will require an internet connection to download. Later runs won't need an internet connection." />
      <item value="In English, tokens commonly range in length from one character to one word (e.g., `&quot;t&quot;` or `&quot; great&quot;`), though in some languages tokens can be shorter than one character or longer than one word. Spaces are usually grouped with the starts of words (e.g., `&quot; is&quot;` instead of `&quot;is &quot;` or `&quot; &quot;`+`&quot;is&quot;`). You can quickly check how a string is tokenized at the [OpenAI Tokenizer](https:beta.openai.comtokenizer)." />
      <item value="How strings are typically tokenized" />
      <item value="(OpenAI makes no endorsements or guarantees of third-party libraries.)" />
      <item value="Tokenizer libraries by language" />
      <item value="Note that `p50k_base` overlaps substantially with `r50k_base`, and for non-code applications, they will usually give the same tokens." />
      <item value="You can retrieve the encoding for a model using `tiktoken.encoding_for_model()` as follows:" />
      <item value="Robust Question Answering with Chroma and OpenAI This notebook guides you step-by-step through answering questions about a collection of data, using [Chroma](https:trychroma.com), an open-source embeddings database, along with OpenAI's [text embeddings](https:platform.openai.comdocsguidesembeddingsuse-cases) and [chat completion](https:platform.openai.comdocsguideschat) API's. Additionally, this notebook demonstrates some of the tradeoffs in making a question answering system more robust. As we shall see, simple querying doesn't always create the best results! Question Answering with LLMs Large language models (LLMs) like OpenAI's ChatGPT can be used to answer questions about data that the model may not have been trained on, or have access to. For example; - Personal data like e-mails and notes - Highly specialized data like archival or legal documents - Newly created data like recent news stories In order to overcome this limitation, we can use a data store which is amenable to querying in natural language, just like the LLM itself. An embeddings store like Chroma represents documents as [embeddings](https:openai.comblogintroducing-text-and-code-embeddings), alongside the documents themselves. By embedding a text query, Chroma can find relevant documents, which we can then pass to the LLM to answer our question. We'll show detailed examples and variants of this approach." />
      <item value="how to spell how grammar works how to paraphrase how to answer questions how to hold a conversation how to write in many languages how to code etc." />
      <item value="How to work with large language models How large language models work [Large language models][Large language models Blog Post] are functions that map text to text. Given an input string of text, a large language model predicts the text that should come next. The magic of large language models is that by being trained to minimize this prediction error over vast quantities of text, the models end up learning concepts useful for these predictions. For example, they learn: how to spell how grammar works how to paraphrase how to answer questions how to hold a conversation how to write in many languages how to code etc. None of these capabilities are explicitly programmed in—they all emerge as a result of training. GPT-3 powers [hundreds of software products][GPT3 Apps Blog Post], including productivity apps, education apps, games, and more. How to control a large language model Of all the inputs to a large language model, by far the most influential is the text prompt. Large language models can be prompted to produce output in a few ways: Instruction: Tell the model what you want Completion: Induce the model to complete the beginning of what you want Demonstration: Show the model what you want, with either: A few examples in the prompt Many hundreds or thousands of examples in a fine-tuning training dataset An example of each is shown below. Instruction prompts Instruction-following models (e.g., `text-davinci-003` or any model beginning with `text-`) are specially designed to follow instructions. Write your instruction at the top of the prompt (or at the bottom, or both), and the model will do its best to follow the instruction and then stop. Instructions can be detailed, so don't be afraid to write a paragraph explicitly detailing the output you want. Example instruction prompt: ```text Extract the name of the author from the quotation below. “Some humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.” ― Ted Chiang, Exhalation ``` Output: ```text Ted Chiang ``` Completion prompt example Completion-style prompts take advantage of how large language models try to write text they think is mostly likely to come next. To steer the model, try beginning a pattern or sentence that will be completed by the output you want to see. Relative to direct instructions, this mode of steering large language models can take more care and experimentation. In addition, the models won't necessarily know where to stop, so you will often need stop sequences or post-processing to cut off text generated beyond the desired output. Example completion prompt: ```text “Some humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.” ― Ted Chiang, Exhalation The author of this quote is ``` Output: ```text Ted Chiang ``` Demonstration prompt example (few-shot learning) Similar to completion-style prompts, demonstrations can show the model what you want it to do. This approach is sometimes called few-shot learning, as the model learns from a few examples provided in the prompt. Example demonstration prompt: ```text Quote: “When the reasoning mind is forced to confront the impossible again and again, it has no choice but to adapt.” ― N.K. Jemisin, The Fifth Season Author: N.K. Jemisin Quote: “Some humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.” ― Ted Chiang, Exhalation Author: ``` Output: ```text Ted Chiang ``` Fine-tuned prompt example With enough training examples, you can [fine-tune][Fine Tuning Docs] a custom model. In this case, instructions become unnecessary, as the model can learn the task from the training data provided. However, it can be helpful to include separator sequences (e.g., `-&gt;` or `` or any string that doesn't commonly appear in your inputs) to tell the model when the prompt has ended and the output should begin. Without separator sequences, there is a risk that the model continues elaborating on the input text rather than starting on the answer you want to see. Example fine-tuned prompt (for a model that has been custom trained on similar prompt-completion pairs): ```text “Some humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.” ― Ted Chiang, Exhalation ``` Output: ```text Ted Chiang ``` Code Capabilities Large language models aren't only great at text - they can be great at code too. OpenAI's specialized code model is called [Codex]. Codex powers [more than 70 products][Codex Apps Blog Post], including: [GitHub Copilot] (autocompletes code in VS Code and other IDEs) [Pygma](https:pygma.app) (turns Figma designs into code) [Replit](https:replit.com) (has an 'Explain code' button and other features) [Warp](https:www.warp.dev) (a smart terminal with AI command search) [Machinet](https:machinet.net) (writes Java unit test templates) Note that unlike instruction-following text models (e.g., `text-davinci-002`), Codex is not trained to follow instructions. As a result, designing good prompts can take more care. More prompt advice For more prompt examples, visit [OpenAI Examples][OpenAI Examples]. In general, the input prompt is the best lever for improving model outputs. You can try tricks like: Give more explicit instructions. E.g., if you want the output to be a comma separated list, ask it to return a comma separated list. If you want it to say &quot;I don't know&quot; when it doesn't know the answer, tell it 'Say &quot;I don't know&quot; if you do not know the answer.' Supply better examples. If you're demonstrating examples in your prompt, make sure that your examples are diverse and high quality. Ask the model to answer as if it was an expert. Explicitly asking the model to produce high quality output or output as if it was written by an expert can induce the model to give higher quality answers that it thinks an expert would write. E.g., &quot;The following answer is correct, high-quality, and written by an expert.&quot; Prompt the model to write down the series of steps explaining its reasoning. E.g., prepend your answer with something like &quot;[Let's think step by step](https:arxiv.orgpdf2205.11916v1.pdf).&quot; Prompting the model to give an explanation of its reasoning before its final answer can increase the likelihood that its final answer is consistent and correct. [Fine Tuning Docs]: https:beta.openai.comdocsguidesfine-tuning [Codex Apps Blog Post]: https:openai.comblogcodex-apps [Large language models Blog Post]: https:openai.comblogbetter-language-models [GitHub Copilot]: https:copilot.github.com [Codex]: https:openai.comblogopenai-codex [GPT3 Apps Blog Post]: https:openai.combloggpt-3-apps [OpenAI Examples]: https:beta.openai.comexamples" />
      <item value="Change these to take in a file rather than pasted code, if non-file is given, return instructions &quot;Input should be a python filepath, write your code to file and try again&quot;" />
      <item value="Attempting to fix JSON by finding outermost brackets" />
      <item value="I have received an invalid JSON response from the OpenAI API. Trying to fix it now." />
      <item value="Fixes the provided JSON string to make it parseable&quot;\ &quot; and fully compliant with the provided schema.\n If an object or&quot;\ &quot; field specified in the schema isn't contained within the correct&quot;\ &quot; JSON, it is omitted.\n This function is brilliant at guessing&quot;\ &quot; when the format is incorrect." />
      <item value="Count the currently used tokens" />
      <item value="Add messages from the full message history until we reach the token limit" />
      <item value="This reminds you of these events from your past" />
      <item value="Check if there's a result from the command append it to the message" />
    </histories>
    <option name="languageScores">
      <map>
        <entry key="CHINESE" value="124" />
        <entry key="ENGLISH" value="126" />
      </map>
    </option>
  </component>
</application>