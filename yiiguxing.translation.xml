<application>
  <component name="AppStorage">
    <option name="newTranslationDialogX" value="786" />
    <option name="newTranslationDialogY" value="484" />
    <histories>
      <item value="Content" />
      <item value="supplementary" />
      <item value="compensation" />
      <item value="discrepancies" />
      <item value="There are discrepancies in media consumption, please check your email!" />
      <item value="Media consumption indistinguishable data!" />
      <item value="preamble" />
      <item value="transpose" />
      <item value="embedding" />
      <item value="When the timeout argument is not present or None, the operation will block until the thread terminates. A thread can be join()ed many times. join() raises a RuntimeError if an attempt is made to join the current thread as that would cause a deadlock. It is also an error to join() a thread before it has been started and attempts to do so raises the same exception." />
      <item value="hen the timeout argument is present and not None, it should be a floating point number specifying a timeout for the operation in seconds (or fractions thereof). As join() always returns None, you must call isAlive() after join() to decide whether a timeout happened -- if the thread is still alive, the join() call timed out." />
      <item value="This blocks the calling thread until the thread whose join() method is called terminates -- either normally or through an unhandled exception or until the optional timeout occurs." />
      <item value="statements" />
      <item value="Normally, THttpServer always sends a 200 response. If a handler wants to override this behavior (e.g., to simulate a misconfigured or overloaded web server during testing), it can raise a ResponseException. The function passed to the constructor will be called with the RequestHandler as its only argument. Note that this is irrelevant for ONEWAY requests, as the HTTP response must be sent before the RPC is processed." />
      <item value="propagate" />
      <item value="income" />
      <item value="Domestic" />
      <item value="stats latency" />
      <item value="latency" />
      <item value="accuracy" />
      <item value="A toolset for quality control, evlation and processing of GRID-seq libarary." />
      <item value="known case of datetime.datetime.utcnow &quot;&quot;&quot; Return a new datetime representing UTC day and time." />
      <item value="Unexpected argument&#10;" />
      <item value="Unexpected argument" />
      <item value="The delimiter according which to split the string. None (the default value) means split according to any whitespace, and discard empty strings from the result. maxsplit&#10;" />
      <item value="The delimiter according which to split the string. None (the default value) means split according to any whitespace, and discard empty strings from the result. maxsplit" />
      <item value="Returns the approximate memory footprint an object and all of its contents. Automatically finds the contents of the following builtin containers and their subclasses: tuple, list, deque, dict, set and frozenset. To search other containers, add handlers to iterate over their contents: handlers = {SomeContainerClass: iter, OtherContainerClass: OtherContainerClass.get_elements}" />
      <item value="verbose" />
      <item value="so slow" />
      <item value="Get polyA peaks for given set of depths" />
      <item value="fusion reads" />
      <item value="sorted ids of intervals in cluster(tuples)" />
      <item value="Cluster reads into positive strand and negative strand" />
      <item value="minimal reads to support APAspeaks" />
      <item value="minimal distance between two adjacent APAspeaks" />
      <item value="Shadows name 'split_unit' from outer scope" />
    </histories>
    <option name="languageScores">
      <map>
        <entry key="CHINESE" value="32" />
        <entry key="ENGLISH" value="33" />
      </map>
    </option>
  </component>
  <component name="Cache">
    <option name="lastTrimTime" value="1657510488418" />
  </component>
  <component name="Translation.Cache">
    <option name="lastTrimTime" value="1686723524101" />
  </component>
  <component name="Translation.States">
    <option name="newTranslationDialogX" value="912" />
    <option name="newTranslationDialogY" value="452" />
    <option name="pinTranslationDialog" value="true" />
    <histories>
      <item value="Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer." />
      <item value="Call predict on the LLM." />
      <item value="&quot;&quot;&quot;Run the logic of this chain and add to output if desired. Args: inputs: Dictionary of inputs, or single input if chain expects only one param. return_only_outputs: boolean for whether to return only outputs in the response. If True, only new keys generated by this chain will be returned. If False, both input keys and new keys generated by this chain will be returned. Defaults to False. callbacks: Callbacks to use for this chain run. If not provided, will use the callbacks provided to the chain. include_run_info: Whether to include run info in the response. Defaults to False. &quot;&quot;&quot;" />
      <item value="A rate limit error will occur when API requests are sent too quickly. If using the OpenAI Python library, they will look something like:" />
      <item value="Rate limits are a common practice for APIs, and they're put in place for a few different reasons. - First, they help protect against abuse or misuse of the API. For example, a malicious actor could flood the API with requests in an attempt to overload it or cause disruptions in service. By setting rate limits, OpenAI can prevent this kind of activity. - Second, rate limits help ensure that everyone has fair access to the API. If one person or organization makes an excessive number of requests, it could bog down the API for everyone else. By throttling the number of requests that a single user can make, OpenAI ensures that everyone has an opportunity to use the API without experiencing slowdowns. - Lastly, rate limits can help OpenAI manage the aggregate load on its infrastructure. If requests to the API increase dramatically, it could tax the servers and cause performance issues. By setting rate limits, OpenAI can help maintain a smooth and consistent experience for all users. Although hitting rate limits can be frustrating, rate limits exist to protect the reliable operation of the API for its users." />
      <item value="To see an example script for throttling parallel requests to avoid rate limit errors, see [api_request_parallel_processor.py](api_request_parallel_processor.py)." />
      <item value="This guide shares tips for avoiding and handling rate limit errors." />
      <item value="When you call the OpenAI API repeatedly, you may encounter error messages that say `429: 'Too Many Requests'` or `RateLimitError`. These error messages come from exceeding the API's rate limits." />
      <item value="How to handle rate limits" />
      <item value="Techniques to improve reliability" />
      <item value="VectorStore implementation using Postgres and pgvector. - `connection_string` is a postgres connection string. - `embedding_function` any embedding function implementing `langchain.embeddings.base.Embeddings` interface. - `collection_name` is the name of the collection to use. (default: langchain) - NOTE: This is not the name of the table, but the name of the collection. The tables will be created when initializing the store (if not exists) So, make sure the user has the right permissions to create tables. - `distance_strategy` is the distance strategy to use. (default: EUCLIDEAN) - `EUCLIDEAN` is the euclidean distance. - `COSINE` is the cosine distance. - `pre_delete_collection` if True, will delete the collection if it exists. (default: False) - Useful for testing." />
      <item value="Deprecation of Answers, Classification, and Search" />
      <item value="&quot;&quot;&quot; The preferred method to get a ClickHouse Connect Client instance :param host: The hostname or IP address of the ClickHouse server. If not set, localhost will be used. :param username: The ClickHouse username. If not set, the default ClickHouse user will be used. :param password: The password for username. :param database: The default database for the connection. If not set, ClickHouse Connect will use the default database for username. :param interface: Must be http or https. Defaults to http, or to https if port is set to 8443 or 443 :param port: The ClickHouse HTTP or HTTPS port. If not set will default to 8123, or to 8443 if secure=True or interface=https. :param secure: Use httpsTLS. This overrides inferred values from the interface or port arguments. :param dsn: A string in standard DSN (Data Source Name) format. Other connection values (such as host or user) will be extracted from this string if not set otherwise. :param settings: ClickHouse server settings to be used with the sessionevery request :param generic_args: Used internally to parse DBAPI connection strings into keyword arguments and ClickHouse settings. It is not recommended to use this parameter externally. :param kwargs -- Recognized keyword arguments (used by the HTTP client), see below :param compress: Enable compression for ClickHouse HTTP inserts and query results. True will select the preferred compression method (lz4). A str of 'lz4', 'zstd', 'brotli', or 'gzip' can be used to use a specific compression type :param query_limit: Default LIMIT on returned rows. 0 means no limit :param connect_timeout: Timeout in seconds for the http connection :param send_receive_timeout: Read timeout in seconds for http connection :param client_name: client_name prepended to the HTTP User Agent header. Set this to track client queries in the ClickHouse system.query_log. :param send_progress: Deprecated, has no effect. Previous functionality is now automatically determined :param verify: Verify the server certificate in securehttps mode :param ca_cert: If verify is True, the file path to Certificate Authority root to validate ClickHouse server certificate, in .pem format. Ignored if verify is False. This is not necessary if the ClickHouse server certificate is trusted by the operating system. To trust the maintained list of &quot;global&quot; public root certificates maintained by the Python 'certifi' package, set ca_cert to 'certifi' :param client_cert: File path to a TLS Client certificate in .pem format. This file should contain any applicable intermediate certificates :param client_cert_key: File path to the private key for the Client Certificate. Required if the private key is not included the Client Certificate key file :param session_id ClickHouse session id. If not specified and the common setting 'autogenerate_session_id' is True, the client will generate a UUID1 session id :param pool_mgr Optional urllib3 PoolManager for this client. Useful for creating separate connection pools for multiple client endpoints for applications with many clients :param http_proxy http proxy address. Equivalent to setting the HTTP_PROXY environment variable :param https_proxy https proxy address. Equivalent to setting the HTTPS_PROXY environment variable :param server_host_name This is the server host name that will be checked against a TLS certificate for validity. This option can be used if using an ssh_tunnel or other indirect means to an ClickHouse server where the `host` argument refers to the tunnel or proxy and not the actual ClickHouse server :return: ClickHouse Connect Client instance" />
      <item value="&quot;&quot;&quot;Call out to OpenAI's endpoint with k unique prompts. Args: prompts: The prompts to pass into the model. stop: Optional list of stop words to use when generating. Returns: The full LLM output. Example: .. code-block:: python response = openai.generate([&quot;Tell me a joke.&quot;]) &quot;&quot;&quot; TODO: write a unit test for this" />
      <item value="Remove plural from include since db columns are singular" />
      <item value="Either the collection name or the collection uuid must be provided" />
      <item value="&quot;&quot;&quot;Gets the nearest neighbors of a single embedding ⚠️ This method should not be used directly. Args: embedding (Sequence[float]): The embedding to find the nearest neighbors of n_results (int, optional): The number of nearest neighbors to return. Defaults to 10. where (Dict[str, str], optional): A dictionary of key-value pairs to filter the embeddings by. Defaults to {}. &quot;&quot;&quot;" />
      <item value="Infers if target is Embedding, Metadata, or Document and casts it to a many object if its one" />
      <item value="&quot;&quot;&quot;Get the n_results nearest neighbor embeddings for provided query_embeddings or query_texts. Args: query_embeddings: The embeddings to get the closes neighbors of. Optional. query_texts: The document texts to get the closes neighbors of. Optional. n_results: The number of neighbors to return for each query_embedding or query_text. Optional. where: A Where type dict used to filter results by. E.g. {&quot;color&quot; : &quot;red&quot;, &quot;price&quot;: 4.20}. Optional. where_document: A WhereDocument type dict used to filter by the documents. E.g. {contains: {&quot;text&quot;: &quot;hello&quot;}}. Optional. include: A list of what to include in the results. Can contain &quot;embeddings&quot;, &quot;metadatas&quot;, &quot;documents&quot;, &quot;distances&quot;. Ids are always included. Defaults to [&quot;metadatas&quot;, &quot;documents&quot;, &quot;distances&quot;]. Optional. &quot;&quot;&quot;" />
      <item value="&quot;&quot;&quot;Return docs and relevance scores in the range [0, 1]. 0 is dissimilar, 1 is most similar. Args: query: input text k: Number of Documents to return. Defaults to 4. kwargs: kwargs to be passed to similarity search. Should include: score_threshold: Optional, a floating point value between 0 to 1 to filter the resulting set of retrieved docs Returns: List of Tuples of (doc, similarity_score) &quot;&quot;&quot;" />
      <item value="&quot;&quot;&quot;Run get_relevant_text and llm on input query. If chain has 'return_source_documents' as 'True', returns the retrieved documents as well under the key 'source_documents'. Example: .. code-block:: python res = indexqa({'query': 'This is my query'}) answer, docs = res['result'], res['source_documents'] &quot;&quot;&quot;" />
      <item value="generate" />
      <item value="Use the following pieces of context to answer the users question. If you don't know the answer, just say that you don't know, don't try to make up an answer. ---------------- {context}" />
      <item value="&quot;&quot;&quot;Load question answering chain. Args: llm: Language Model to use in the chain. chain_type: Type of document combining chain to use. Should be one of &quot;stuff&quot;, &quot;map_reduce&quot;, &quot;map_rerank&quot;, and &quot;refine&quot;. verbose: Whether chains should be run in verbose mode or not. Note that this applies to all chains that make up the final chain. callback_manager: Callback manager to use for the chain. Returns: A chain to use for question answering. &quot;&quot;&quot;" />
      <item value="&quot;You are trying to use a chat model. This way of initializing it is &quot; &quot;no longer supported. Instead, please use: &quot; &quot;`from langchain.chat_models import ChatOpenAI`&quot;" />
      <item value="&quot;&quot;&quot;Wrapper around Azure-specific OpenAI large language models. To use, you should have the ``openai`` python package installed, and the environment variable ``OPENAI_API_KEY`` set with your API key. Any parameters that are valid to be passed to the openai.create call can be passed in, even if not explicitly saved on this class. Example: .. code-block:: python from langchain.llms import AzureOpenAI openai = AzureOpenAI(model_name=&quot;text-davinci-003&quot;) &quot;&quot;&quot;" />
      <item value="&quot;&quot;&quot;Get documents relevant for a query. Args: query: string to find relevant documents for Returns: List of relevant documents &quot;&quot;&quot;" />
      <item value="&quot;&quot;&quot;Chain for question-answering against an index. Example: .. code-block:: python from langchain.llms import OpenAI from langchain.chains import RetrievalQA from langchain.faiss import FAISS from langchain.vectorstores.base import VectorStoreRetriever retriever = VectorStoreRetriever(vectorstore=FAISS(...)) retrievalQA = RetrievalQA.from_llm(llm=OpenAI(), retriever=retriever) &quot;&quot;&quot;" />
      <item value="Chain for question-answering against a vector database" />
      <item value="should this return the ids of the succesfully added items?" />
      <item value="replace newlines, which can negatively affect performance." />
      <item value="Use tenacity to retry the embedding call" />
      <item value="The chunk size of embeddings. If None, will use the chunk size specified by the class." />
      <item value="Returns: List[str]: List of IDs of the added texts." />
      <item value="Args: texts (Iterable[str]): Texts to add to the vectorstore. metadatas (Optional[List[dict]], optional): Optional list of metadatas. ids (Optional[List[str]], optional): Optional list of IDs." />
      <item value="Run more texts through the embeddings and add to the vectorstore." />
      <item value="&quot;&quot;&quot;Run more texts through the embeddings and add to the vectorstore. Args: texts (Iterable[str]): Texts to add to the vectorstore. metadatas (Optional[List[dict]], optional): Optional list of metadatas. ids (Optional[List[str]], optional): Optional list of IDs. Returns: List[str]: List of IDs of the added texts. &quot;&quot;&quot; TODO: Handle the case where the user doesn't provide ids on the Collection" />
      <item value="TODO: inherits ClickHouse for convenience of copying behavior, not because it's logically a subtype. Factoring out the common behavior to a third superclass they both extend would be preferable." />
      <item value="poor man's unique constraint" />
      <item value="Create a Chroma vectorstore from a raw documents. If a persist_directory is specified, the collection will be persisted there. Otherwise, the data will be ephemeral in-memory. Args: texts (List[str]): List of texts to add to the collection. collection_name (str): Name of the collection to create. persist_directory (Optional[str]): Directory to persist the collection. embedding (Optional[Embeddings]): Embedding function. Defaults to None. metadatas (Optional[List[dict]]): List of metadatas. Defaults to None. ids (Optional[List[str]]): List of document IDs. Defaults to None. client_settings (Optional[chromadb.config.Settings]): Chroma client settings Returns: Chroma: Chroma vectorstore." />
      <item value="Maximum number of texts to embed in each batch" />
      <item value="Wrapper around OpenAI embedding models. To use, you should have the ``openai`` python package installed, and the environment variable ``OPENAI_API_KEY`` set with your API key or pass it as a named parameter to the constructor. Example: .. code-block:: python from langchain.embeddings import OpenAIEmbeddings openai = OpenAIEmbeddings(openai_api_key=&quot;my-api-key&quot;) In order to use the library with Microsoft Azure endpoints, you need to set the OPENAI_API_TYPE, OPENAI_API_BASE, OPENAI_API_KEY and optionally and API_VERSION. The OPENAI_API_TYPE must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the model parameter. Example: .. code-block:: python import os os.environ[&quot;OPENAI_API_TYPE&quot;] = &quot;azure&quot; os.environ[&quot;OPENAI_API_BASE&quot;] = &quot;https:&lt;your-endpoint.openai.azure.com&quot; os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;your AzureOpenAI key&quot; from langchain.embeddings.openai import OpenAIEmbeddings embeddings = OpenAIEmbeddings( deployment=&quot;your-embeddings-deployment-name&quot;, model=&quot;your-embeddings-model-name&quot; ) text = &quot;This is a test query.&quot; query_result = embeddings.embed_query(text)" />
      <item value="The Chatbot Kickstarter is a starter repo to get you used to building basic a basic Chatbot using the ChatGPT API and your own knowledge base. The flow you're taken through was originally presented with [these slides](https:drive.google.comfiled1dB-RQhZC_Q1iAsHkNNdkqtxxXqYODFYyview?usp=share_link), which may come in useful to refer to." />
      <item value="The Chatbot Kickstarter is a starter repo to get you used to building basic a basic Chatbot using the ChatGPT API and your own knowledge base. The flow you're taken through was originally presented with [these slides](https:drive.google." />
      <item value="The expected output is the translated chunk of text." />
      <item value="The format of the prompt sent to the model consists of: 1. A high level instruction to translate only the text, but not commands into the desired language 2. A sample untranslated command, where only the content of the chapter name needs to be translated 3. The chunk of text to be translated 4. The translated sample command from 2, which shows the model the beginning of the translation process" />
      <item value="Notice that adding a sample untranslated and translated first command, where only the content of the chapter name needs to be translated, helps to get more consistent results." />
      <item value="if adding this chunk would exceed the max length, finalize the current batch and start a new one" />
      <item value="Group very short chunks, to form approximately a page long chunks." />
      <item value="It turns out that a double newline is a good separator in this case, in order not to break the flow of the text. Also no individual chunk is larger than 1500 tokens. The model we will use is text-davinci-002, which has a limit of 4096 tokens, so we don't need to worry about breaking the chunks down further. We will group the shorter chunks into chunks of around 1000 tokens, to increase the coherence of the text, and decrease the frequency of breaks within the text." />
    </histories>
    <option name="languageScores">
      <map>
        <entry key="CHINESE" value="156" />
        <entry key="ENGLISH" value="159" />
      </map>
    </option>
  </component>
</application>